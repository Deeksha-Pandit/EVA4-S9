{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "S7.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d51c4c42748d40c1b2d0644d3c92a077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c86304c988044aebb60c4f0acfa65163",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c55b83f7c271417f9140866863db5daf",
              "IPY_MODEL_ff609964eb0e48ce9871d80ffb2c0451"
            ]
          }
        },
        "c86304c988044aebb60c4f0acfa65163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c55b83f7c271417f9140866863db5daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_07e88278dd434851be07fd82b63c8dba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a7358150f2a4d02920a3f6f87bcb795"
          }
        },
        "ff609964eb0e48ce9871d80ffb2c0451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5453a7f8aa6e4b36972c0f9ddb12de67",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:03, 43833165.12it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3fd637003d240f9bea94b827b3dcf67"
          }
        },
        "07e88278dd434851be07fd82b63c8dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a7358150f2a4d02920a3f6f87bcb795": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5453a7f8aa6e4b36972c0f9ddb12de67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3fd637003d240f9bea94b827b3dcf67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deeksha-Pandit/EVA4-S9/blob/master/S9_withAlbumentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVbdg0z4poCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpE8dMzmcOA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "outputId": "8248c6cf-3542-4968-dc56-6b544c79bee4"
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-ahyj_gbh\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-ahyj_gbh\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.4.1)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (3.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (45.2.0)\n",
            "Building wheels for collected packages: albumentations, imgaug\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.5-cp36-none-any.whl size=64514 sha256=e68fce1f8ca5ae148347cebb41d12b9c5b54b4744c7d0a51490e68aedb52aeab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-crzma4_i/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=6d3346ab7a6012cc88d956679632a5c78bc701e7e00dc6b15b5b1ec14c5a3be4\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built albumentations imgaug\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.5 imgaug-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPogbueepoCh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Training a Classifier\n",
        "=====================\n",
        "\n",
        "This is it. You have seen how to define neural networks, compute loss and make\n",
        "updates to the weights of the network.\n",
        "\n",
        "Now you might be thinking,\n",
        "\n",
        "What about data?\n",
        "----------------\n",
        "\n",
        "Generally, when you have to deal with image, text, audio or video data,\n",
        "you can use standard python packages that load data into a numpy array.\n",
        "Then you can convert this array into a ``torch.*Tensor``.\n",
        "\n",
        "-  For images, packages such as Pillow, OpenCV are useful\n",
        "-  For audio, packages such as scipy and librosa\n",
        "-  For text, either raw Python or Cython based loading, or NLTK and\n",
        "   SpaCy are useful\n",
        "\n",
        "Specifically for vision, we have created a package called\n",
        "``torchvision``, that has data loaders for common datasets such as\n",
        "Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,\n",
        "``torchvision.datasets`` and ``torch.utils.data.DataLoader``.\n",
        "\n",
        "This provides a huge convenience and avoids writing boilerplate code.\n",
        "\n",
        "For this tutorial, we will use the CIFAR10 dataset.\n",
        "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
        "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of\n",
        "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
        "\n",
        ".. figure:: /_static/img/cifar10.png\n",
        "   :alt: cifar10\n",
        "\n",
        "   cifar10\n",
        "\n",
        "\n",
        "Training an image classifier\n",
        "----------------------------\n",
        "\n",
        "We will do the following steps in order:\n",
        "\n",
        "1. Load and normalizing the CIFAR10 training and test datasets using\n",
        "   ``torchvision``\n",
        "2. Define a Convolution Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data\n",
        "\n",
        "1. Loading and normalizing CIFAR10\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "Using ``torchvision``, it’s extremely easy to load CIFAR10.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yVpodAMpoCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from albumentations import Compose, Normalize, HorizontalFlip\n",
        "from albumentations.pytorch import ToTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kjt4e2TpoCk",
        "colab_type": "text"
      },
      "source": [
        "The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "We transform them to Tensors of normalized range [-1, 1].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Tjigo2poCl",
        "colab_type": "code",
        "outputId": "8ba3bdfa-01fa-4288-dba1-76c965fdca81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "d51c4c42748d40c1b2d0644d3c92a077",
            "c86304c988044aebb60c4f0acfa65163",
            "c55b83f7c271417f9140866863db5daf",
            "ff609964eb0e48ce9871d80ffb2c0451",
            "07e88278dd434851be07fd82b63c8dba",
            "1a7358150f2a4d02920a3f6f87bcb795",
            "5453a7f8aa6e4b36972c0f9ddb12de67",
            "e3fd637003d240f9bea94b827b3dcf67"
          ]
        }
      },
      "source": [
        "class album_Compose:\n",
        "  def __init__(self, train=False):\n",
        "    self.train=train\n",
        "    self.albumentations_transform_train = Compose([\n",
        "                                            # Resize(256,256),\n",
        "                                            # RandomCrop(224,224),\n",
        "                                             HorizontalFlip(),\n",
        "                                             Normalize(0.5,0.5,0.5\n",
        "                                                 #mean=[0.485,0.456,0.406],\n",
        "                                                 #std=[0.229,0.224,0.225]\n",
        "                                             ),\n",
        "                                             ToTensor()   \n",
        "    ])\n",
        "    self.albumentations_transform_test = Compose([\n",
        "                                             Normalize(0.5,0.5,0.5\n",
        "                                                 #mean=[0.485,0.456,0.406],\n",
        "                                                 #std=[0.229,0.224,0.225]\n",
        "                                             ),\n",
        "                                             ToTensor()   \n",
        "    ])\n",
        "  def __call__(self,img):\n",
        "    img = np.array(img)\n",
        "    if self.train:\n",
        "      img = self.albumentations_transform_train(image = img)['image']\n",
        "    else:\n",
        "      img = self.albumentations_transform_test(image = img)['image']\n",
        "    return img\n",
        "\n",
        "\"\"\"transform = transforms.Compose(\n",
        "    [transforms.RandomRotation(20),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\"\"\"\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "Batch_size = 64\n",
        "if cuda:\n",
        "  Batch_size = 128\n",
        "  \n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=Batch_size, num_workers=8, pin_memory=True) if cuda else dict(shuffle=True, batch_size=2)\n",
        "train_compose = album_Compose(train=True)\n",
        "test_compose = album_Compose()\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_compose)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, **dataloader_args)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_compose)\n",
        "testloader = torch.utils.data.DataLoader(testset, **dataloader_args)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d51c4c42748d40c1b2d0644d3c92a077",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cj433rtpoCn",
        "colab_type": "text"
      },
      "source": [
        "Let us show some of the training images, for fun.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daA7bADhpoCo",
        "colab_type": "code",
        "outputId": "df12eb79-2c89-4dee-a1bf-3a990a79902c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "plane   dog  deer  deer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJUAAAD8CAYAAAB+WebdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28OpHA_LpoCq",
        "colab_type": "text"
      },
      "source": [
        "2. Define a Convolution Neural Network\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "Copy the neural network from the Neural Networks section before and modify it to\n",
        "take 3-channel images (instead of 1-channel images as it was defined).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jjLwLLQpoCr",
        "colab_type": "code",
        "outputId": "4098dc36-fc95-4db4-86b7-e1a8d1169c0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1,3,32,32))\n",
        "    print(y.size())\n",
        "test()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS2ysbfz9NJX",
        "colab_type": "code",
        "outputId": "f85c1802-9ae8-46bb-8d27-df635b178b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "resnet = ResNet18().to(device)\n",
        "summary(resnet, input_size=(3, 32, 32))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
            "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
            "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
            "           Conv2d-13          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-14          [-1, 128, 16, 16]             256\n",
            "           Conv2d-15          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
            "           Conv2d-17          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-19          [-1, 128, 16, 16]               0\n",
            "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
            "           Conv2d-22          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-24          [-1, 128, 16, 16]               0\n",
            "           Conv2d-25            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-26            [-1, 256, 8, 8]             512\n",
            "           Conv2d-27            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-28            [-1, 256, 8, 8]             512\n",
            "           Conv2d-29            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-30            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-31            [-1, 256, 8, 8]               0\n",
            "           Conv2d-32            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-33            [-1, 256, 8, 8]             512\n",
            "           Conv2d-34            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-35            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-36            [-1, 256, 8, 8]               0\n",
            "           Conv2d-37            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-39            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-41            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-43            [-1, 512, 4, 4]               0\n",
            "           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-46            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-48            [-1, 512, 4, 4]               0\n",
            "           Linear-49                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,173,962\n",
            "Trainable params: 11,173,962\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 11.25\n",
            "Params size (MB): 42.63\n",
            "Estimated Total Size (MB): 53.89\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdw2NNCUpoCu",
        "colab_type": "text"
      },
      "source": [
        "3. Define a Loss function and optimizer\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4xBbDROpoCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=8, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-odOrhdpoC2",
        "colab_type": "text"
      },
      "source": [
        "4. Train the network\n",
        "^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "This is when things start to get interesting.\n",
        "We simply have to loop over our data iterator, and feed the inputs to the\n",
        "network and optimize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkuRfo1IpoC3",
        "colab_type": "code",
        "outputId": "fed18080-696b-4d66-e26e-73952891d06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "minibatch= int(len(trainset)/Batch_size)\n",
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs,labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = resnet(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        pred = outputs.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        processed += len(inputs)\n",
        "\n",
        "        if i == minibatch:    # print every mini-batches\n",
        "            print('[%d, %5d] loss: %.3f,Acc: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000,100*correct/processed))\n",
        "            running_loss = 0.0\n",
        "    scheduler.step()\n",
        "print('Finished Training')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   391] loss: 0.249,Acc: 53.822\n",
            "[2,   391] loss: 0.148,Acc: 73.456\n",
            "[3,   391] loss: 0.112,Acc: 80.142\n",
            "[4,   391] loss: 0.089,Acc: 84.132\n",
            "[5,   391] loss: 0.072,Acc: 87.180\n",
            "[6,   391] loss: 0.060,Acc: 89.300\n",
            "[7,   391] loss: 0.049,Acc: 91.300\n",
            "[8,   391] loss: 0.041,Acc: 92.700\n",
            "[9,   391] loss: 0.020,Acc: 96.864\n",
            "[10,   391] loss: 0.013,Acc: 98.098\n",
            "[11,   391] loss: 0.011,Acc: 98.516\n",
            "[12,   391] loss: 0.008,Acc: 98.970\n",
            "[13,   391] loss: 0.007,Acc: 99.208\n",
            "[14,   391] loss: 0.006,Acc: 99.460\n",
            "[15,   391] loss: 0.005,Acc: 99.576\n",
            "[16,   391] loss: 0.003,Acc: 99.760\n",
            "[17,   391] loss: 0.003,Acc: 99.834\n",
            "[18,   391] loss: 0.003,Acc: 99.848\n",
            "[19,   391] loss: 0.003,Acc: 99.852\n",
            "[20,   391] loss: 0.003,Acc: 99.862\n",
            "[21,   391] loss: 0.003,Acc: 99.872\n",
            "[22,   391] loss: 0.002,Acc: 99.900\n",
            "[23,   391] loss: 0.003,Acc: 99.900\n",
            "[24,   391] loss: 0.002,Acc: 99.876\n",
            "[25,   391] loss: 0.002,Acc: 99.906\n",
            "[26,   391] loss: 0.002,Acc: 99.898\n",
            "[27,   391] loss: 0.002,Acc: 99.888\n",
            "[28,   391] loss: 0.002,Acc: 99.900\n",
            "[29,   391] loss: 0.002,Acc: 99.906\n",
            "[30,   391] loss: 0.002,Acc: 99.910\n",
            "[31,   391] loss: 0.002,Acc: 99.912\n",
            "[32,   391] loss: 0.002,Acc: 99.904\n",
            "[33,   391] loss: 0.002,Acc: 99.898\n",
            "[34,   391] loss: 0.002,Acc: 99.912\n",
            "[35,   391] loss: 0.002,Acc: 99.888\n",
            "[36,   391] loss: 0.002,Acc: 99.904\n",
            "[37,   391] loss: 0.002,Acc: 99.894\n",
            "[38,   391] loss: 0.002,Acc: 99.902\n",
            "[39,   391] loss: 0.002,Acc: 99.914\n",
            "[40,   391] loss: 0.002,Acc: 99.920\n",
            "[41,   391] loss: 0.002,Acc: 99.904\n",
            "[42,   391] loss: 0.002,Acc: 99.924\n",
            "[43,   391] loss: 0.002,Acc: 99.904\n",
            "[44,   391] loss: 0.002,Acc: 99.912\n",
            "[45,   391] loss: 0.002,Acc: 99.918\n",
            "[46,   391] loss: 0.002,Acc: 99.914\n",
            "[47,   391] loss: 0.002,Acc: 99.902\n",
            "[48,   391] loss: 0.002,Acc: 99.892\n",
            "[49,   391] loss: 0.002,Acc: 99.922\n",
            "[50,   391] loss: 0.002,Acc: 99.906\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLrvL_fHpoC5",
        "colab_type": "text"
      },
      "source": [
        "5. Test the network on the test data\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "We have trained the network for 2 passes over the training dataset.\n",
        "But we need to check if the network has learnt anything at all.\n",
        "\n",
        "We will check this by predicting the class label that the neural network\n",
        "outputs, and checking it against the ground-truth. If the prediction is\n",
        "correct, we add the sample to the list of correct predictions.\n",
        "\n",
        "Okay, first step. Let us display an image from the test set to get familiar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nizbsvw9poC6",
        "colab_type": "code",
        "outputId": "3d0d3b93-6132-45dd-bcb2-ebc42735278a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GroundTruth:   bird  deer  ship plane\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJUAAAD8CAYAAAB+WebdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBJXBdUxQGVzHBJXBdUxQGVzHBJXBdf4/YFpy2naKkZoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAx4QPHVpoC8",
        "colab_type": "text"
      },
      "source": [
        "Okay, now let us see what the neural network thinks these examples above are:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVLjlssWpoC9",
        "colab_type": "code",
        "outputId": "c6a39451-72de-4d93-abbd-270747c39009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "outputs = resnet(images.to(device))\n",
        "print(outputs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.7536, -2.5451,  0.3573,  ..., -2.4004,  1.9162, -2.1598],\n",
            "        [-4.0349, -6.2003,  5.6756,  ..., -1.2937, -3.5701, -4.2041],\n",
            "        [ 5.5849,  0.8479, -1.1494,  ..., -2.7838,  3.0653,  4.4294],\n",
            "        ...,\n",
            "        [11.8367, -7.5381, 13.9829,  ..., -8.9892, -1.1098, -4.0602],\n",
            "        [-0.6282, -0.1549,  4.1636,  ..., -4.3266,  1.1082, -2.3210],\n",
            "        [-2.7901, -0.3899,  0.0522,  ..., -1.2542, -1.8066, -4.7362]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8OTHZEApoC_",
        "colab_type": "text"
      },
      "source": [
        "The outputs are energies for the 10 classes.\n",
        "Higher the energy for a class, the more the network\n",
        "thinks that the image is of the particular class.\n",
        "So, let's get the index of the highest energy:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABRIDW3lpoDA",
        "colab_type": "code",
        "outputId": "a2a647cc-088c-426a-a399-212d95274122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted:   deer  deer plane plane\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TV0TXL1poDC",
        "colab_type": "text"
      },
      "source": [
        "The results seem pretty good.\n",
        "\n",
        "Let us look at how the network performs on the whole dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ4-5CESpoDD",
        "colab_type": "code",
        "outputId": "fe370da5-ca50-411a-d6b6-2fc855d028d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = resnet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %.3f %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 87.380 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDdwPBl8poDF",
        "colab_type": "text"
      },
      "source": [
        "That looks waaay better than chance, which is 10% accuracy (randomly picking\n",
        "a class out of 10 classes).\n",
        "Seems like the network learnt something.\n",
        "\n",
        "Hmmm, what are the classes that performed well, and the classes that did\n",
        "not perform well:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDNFuDEDpoDG",
        "colab_type": "code",
        "outputId": "33a07f3f-de38-4624-c69f-17b595a190f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = resnet(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 94 %\n",
            "Accuracy of   car : 100 %\n",
            "Accuracy of  bird : 90 %\n",
            "Accuracy of   cat : 75 %\n",
            "Accuracy of  deer : 90 %\n",
            "Accuracy of   dog : 79 %\n",
            "Accuracy of  frog : 82 %\n",
            "Accuracy of horse : 80 %\n",
            "Accuracy of  ship : 94 %\n",
            "Accuracy of truck : 91 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV7S9nXuEtHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}